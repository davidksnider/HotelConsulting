---
title: "HotelsFinalAnalysis"
author: "Walker Burgin, Tara Ghorpadkar, David Snider, Sid Vanam"
date: "7/27/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```


```{r,include=F}
library(tidyverse)
library(countrycode)
library(rvest)
library(dplyr)
library(tidyr)
library(xtable)
library(modelr)
library(broom)
library(class)
library(lubridate)
library(pracma)
library(class)
library(randomForest)


df = read_csv("hotel_booking.csv")
k_df = read_csv("k_df.csv")
rf_stats = read_csv("rf_stats.csv")
```

<center>

![Vila Vita Resort in Algarve, Portugal. Provided by The Leading Hotels of the World.](lw1306_82442005_720x450.jpg)

</center>

# Introduction

Our group used data describing hotel bookings from two hotels in Portugal to answer questions relevant to the travel industry. Our first goal was to predict room rates with confidence. Such a tool would be valuable for travel agencies, hotels, and customers. Although hotels can provide rates only for a finite number of months, a model that predicts future rates may enable travel agencies to book vacations further into the future. Using this model, travel agencies would better serve as a middleman by providing more utility to customers and hotels. Customers could have more booking options with greater price certainty, while hotels could increase ease of transaction. Hotels could also understand seasonal and cyclical consumer demand patterns earlier and thus prepare their budget accordingly.

Our second goal was to predict whether the customer would cancel their booking. In our dataset, about one-third (0.37) of bookings ended up getting canceled. Hotels that can accurately predict customer cancellation can proactively modify operations. Hotels could double book rooms that have high cancelation to try and still make a profit. Alternatively, predicting canceled books can help customers. For example, when asking if a customer wishes to join a waitlist, the hotel can inform them of the likelihood of cancellation or the probability of getting that room. 

# Data

```{r, include=F}
#data cleaning
cleaned1 = df %>% 
  filter(is_canceled == 0) %>% 
  dplyr::select(
    hotel,
    arrival_date_year, 
    arrival_date_month, 
    arrival_date_day_of_month,
    stays_in_week_nights, 
    stays_in_weekend_nights, 
    adr) %>% 
  mutate(length_of_stay = stays_in_week_nights + stays_in_weekend_nights) %>% 
  arrange(
    arrival_date_year, 
    arrival_date_month, 
    arrival_date_day_of_month) %>%
  dplyr::select(
    arrival_date_year, 
    arrival_date_month, 
    arrival_date_day_of_month, 
    length_of_stay, 
    adr, 
    hotel) 
cleaned1$arrival_date_month = as.integer(
  factor(cleaned1$arrival_date_month, 
         levels = month.name))
cleaned1 = cleaned1 %>%
  unite("arrival_date", 
        c("arrival_date_year", 
          "arrival_date_month", 
          "arrival_date_day_of_month"), 
        sep = "/") %>% 
  filter(adr != 0.00)

city_hotels2 = cleaned1 %>% 
  filter(hotel == "City Hotel")
resort_hotels2 = cleaned1 %>%
  filter(hotel == "Resort Hotel")

# city_hotels2 = city_hotels2 %>% 
#   dplyr::select(
#     arrival_date_year, 
#     arrival_date_month, 
#     arrival_date_day_of_month, 
#     length_of_stay, 
#     adr) %>% 
#   unite("arrival_date", 
#         c("arrival_date_year", 
#           "arrival_date_month", 
#           "arrival_date_day_of_month"), 
#         sep = "/") %>% 
#   filter(adr != 0.00)

city_hotels2$arrival_date <- as.Date(city_hotels2$arrival_date)
city_hotels2 = city_hotels2 %>% 
  arrange(arrival_date) %>% 
  group_by(arrival_date) %>% 
  summarise(
    avg_adr = mean(adr)
    )
resort_hotels2$arrival_date <- as.Date(resort_hotels2$arrival_date)
resort_hotels2 = resort_hotels2 %>% 
  arrange(arrival_date) %>% 
  group_by(arrival_date) %>% 
  summarise(
    avg_adr = mean(adr)
    )

model_data_city = city_hotels2
start_date = as.Date("2015-07-01")
model_data_city$arrival_date <- as.numeric(
  difftime(model_data_city$arrival_date, 
           start_date, 
           unit = "days"))
NumDays.city <- model_data_city$arrival_date

model_data_resort = resort_hotels2
model_data_resort$arrival_date <- as.numeric(
  difftime(model_data_resort$arrival_date, 
           start_date, 
           unit = "days"))
NumDays.resort <- model_data_resort$arrival_date
```

The dataset contains 119,319 bookings for two hotels located in Portugal over about two years. There are 32 variables included in the data set; our research questions concerned a select few. **Our first question uses `"arrival_date"` to predict the variable `"avg_adr"`.** Our group combined the variables `"arrival_date_year"`, `"arrival_date_month"`, and `"arrival_date_day_of_month"` into the variable `"arrival_date"`, to record the client's date of arrival in elapsed days since July 1, 2015 (the earliest arrival date). For modeling, we converted `"arrival_date"` into a Date type, and used a function to obtain days elapsed from 07/01/2015.  We created the variable `"avg_adr"` using the variable `"adr"`. `"adr"` is the client's total transaction costs divided by their staying days. In creating `"avg_adr"`, we calculated the mean ADR value for one day for each hotel. After establishing these variables, one can easily observe the increasing oscillating relationship between `"avg_adr"` and `"arrival_date"`, which motivated our first question. 

```{r, echo=F, results="asis"}
#Table for Q1
q1table = model_data_city %>% 
  head(4) %>%
  mutate(arrival_date = as.integer(arrival_date)) %>%
  xtable(
    align=c("c","c","c"),
    type="html",
    caption = "Table 1: Example Data for Q1")
print(q1table, 
      "html", 
      caption.placement = "top",
      scalebox = 0.7,
      include.rownames=FALSE,
      html.table.attributes="align='center',
                             rules='rows',
                             width=50%,
                             frame='hsides',
                             border-spacing=5px")

```

**Our second question predicts the variable `"is_canceled"`**, which has a value of 1 if the booking was canceled, and 0 otherwise. Analysis revealed that certain variables had a statistically significant relationship with is_canceled. The following are the variables we analyzed. `"hotel"` describes whether the observation was at the city hotel or the resort hotel. `"is_repeated_guest"` is a binary variable, valued at 1 if the guest has booked previously. `"lead_time"` describes the number of days between entrance into the booking system and the customer's arrival. `"previous_bookings_not_canceled"` describes the number of the client's previous bookings that were not canceled, while `"previous_cancellations"` describes those that were canceled.

```{r, results="asis", echo=F}
q2table = (df %>% select(
  is_canceled,
  hotel,
  is_repeated_guest,
  adr,
  lead_time,
  previous_bookings_not_canceled,
  previous_cancellations
  ) %>% 
  filter(adr!= 0))[3:7,] %>%
  mutate(
    is_canceled=as.integer(is_canceled),
    is_repeated_guest=as.integer(is_repeated_guest),
    lead_time=as.integer(lead_time),
    previous_bookings_not_canceled = as.integer(previous_bookings_not_canceled),
    previous_cancellations = as.integer(previous_cancellations)
  ) %>%
  xtable(
    align="cccccccc", 
    type="html",
    caption = "Table 2: Example Data for Q2"
    )
print(q2table,
      "html",
      caption.placement = "top",
      scalebox = 0.7,
      include.rownames=FALSE,
      html.table.attributes="align='center',
                             rules='rows',
                             width=50%,
                             frame='hsides',
                             border-spacing=5px")
```

# Results

## Question 1: Can we predict future room rates with confidence?

To answer our first question, which predicts the average daily rate for both the city and the resort hotels for dates not in the dataset, we first built a model on the data that we had in the data set. We noticed that the data trended in an oscillation - the average daily rate per day increased during summer months and decreased during winter months. These peaks and troughs resembled a sinusoidal graph, so we decided to model the data sinusoidally. The data also had a linear component to it as well. From 2015 to 2017, the peaks and troughs also gradually increased, indicating that the hotels were getting slightly more expensive per year, and we combined this as part of our sinusoidal function to create the most accurate model. To represent the sinusoidal nature in a line of best fit for this graph, we first converted the days elapsed into sin and cos values, and then used a fitting function on the data to find the coefficients for the trend line used to model the data in the data set. Once we found the coefficients, we were able to predict the prices for future dates not in the data set. To do this, we created a specific model function, which used the coefficients determined from the previous fit to determine the line of best fit for future dates. We then binded two years’ worth of rows to the original data set - all with adr values set to NA. Using the model function described earlier, we were able to predict average daily rates for the next two years by arrival date. As the trend line shows, the next two years follow the sine curve and also show a linear trend upward in price for both the resort and city hotel, though this upward trend is more pronounced in the city hotel.

The city hotel prediction model indicates that average daily rate per day is higher during summer months than winter months, as the difference in price between the peak price (usually in July) and the lowest price (usually in January) within a year is around 60 - 70 units. However, this difference is much starker in resort hotels. The resort hotels follow a similar pattern, where the average daily rate is higher in summer months than in winter months, but the range between the highest and lowest values within the year is 100 - 125 units. The data is unclear on whether price is measured in Euros (currency of Portugal), or US Dollars, so for analysis of the data, “units” would be a more appropriate term for the measurement of money. This larger difference in resort hotel data is also accompanied by a steeper decline, so that indicates that the price remains high for a shorter amount of time than the city hotels. This also contributes to the fact that city hotel data is more rounded and looks like a series of hills, while the resort data looks sharper and looks more like a series of mountains. This could be due to the fact that resort hotels specifically target customers looking to vacation which usually occurs during summer months. However, city hotels could have vacationers, but could also have customers traveling for work-related or other reasons. Either way, the prediction model helps customers to see when prices will be the highest and to look out for periods where the average daily rate will decrease, and it helps hotel management determine when their revenues from hotel rates will be the highest or lowest and help prepare for when they shift from one specific rate pattern to the next.


```{r, echo=F, figures-side, fig.show="hold", out.width="50%"}
xc <- cos(2*pi*model_data_city$arrival_date/365.25)
xs <- sin(2*pi*model_data_city$arrival_date/365.25)
fit.lm <- lm(avg_adr ~ xc + xs + arrival_date, data = model_data_city)
model_data_city$pred1 <- predict(fit.lm, model_data_city)
model.func = function(day){
  return(
    fit.lm$coefficients[[1]] + fit.lm$coefficients[[2]]*cos(2*pi*day/365.25) + fit.lm$coefficients[[3]]*sin(2*pi*day/365.25) + fit.lm$coefficients[[4]]*day
  )
}
p1 <- ggplot() + 
  geom_point(data = model_data_city, aes(x = arrival_date, y = avg_adr)) + 
  geom_hline(aes(yintercept=0))
p1.trend = p1  + 
  geom_line(data = model_data_city, aes(x = arrival_date, y = pred1), color="red", size = 2)
new = data.frame(arrival_date=793:1577)
model_data_city = bind_rows(model_data_city, new)
model_data_city$pred <- model.func(model_data_city$arrival_date)

#Graph
p1.trend + 
  geom_line(data = model_data_city, 
            aes(x = arrival_date, 
                y = pred)) + 
  geom_vline(xintercept=365.25/2,color="red",linetype=3,size=.5)+
  geom_vline(xintercept=365.25*(3/2),color="red",linetype=3,size=.5)+
  geom_vline(xintercept=365.25*(5/2),color="red",linetype=3,size=.5)+
  geom_vline(xintercept=365.25*(7/2),color="red",linetype=3,size=.5) + xlab("Days elapsed since July 1, 2015") + ylab("Average Daily Rate") + ggtitle("City Hotel Predictions") + 
  theme(plot.title = element_text(hjust = 0.5))

#Resort
xc <- cos(2*pi*model_data_resort$arrival_date/365.25)
xs <- sin(2*pi*model_data_resort$arrival_date/365.25)
fit.lm <- lm(avg_adr ~ xc + xs + arrival_date, data = model_data_resort)
model_data_resort$pred1 <- predict(fit.lm, model_data_resort)
p2 <- ggplot() + geom_point(data =model_data_resort, aes(x = arrival_date, y = avg_adr)) + geom_hline(aes(yintercept=0))
p2.trend = p2  + 
  geom_line(data = model_data_resort, aes(x = arrival_date, y = pred1), color="red", size = 2)
new = data.frame(arrival_date=793:1577)
model_data_resort = bind_rows(model_data_resort, new)
model_data_resort$pred <- model.func(model_data_resort$arrival_date)

#Graph
p2.trend + 
  geom_line(data = model_data_resort, aes(x = arrival_date, y = pred)) + 
  geom_vline(xintercept=365.25/2,color="red",linetype=3,size=.5)+
  geom_vline(xintercept=365.25*(3/2),color="red",linetype=3,size=.5)+
  geom_vline(xintercept=365.25*(5/2),color="red",linetype=3,size=.5)+
  geom_vline(xintercept=365.25*(7/2),color="red",linetype=3,size=.5) + 
  xlab("Days elapsed since July 1, 2015") + ylab("Average Daily Rate") + ggtitle("Resort Hotel Predictions") + 
  theme(plot.title = element_text(hjust = 0.5))
```

The mathematical equation for city hotel data:

$$14.66 \cos({2 \pi x \over 365.25}) + 5.60 \sin({2 \pi x \over 365.25}) + 0.0553x + 81.10$$

The mathematical equation for resort hotel data:

$$46.47 \cos({2 \pi x \over 365.25}) + 26.28 \sin({2 \pi x \over 365.25}) + 0.0486x + 70.13$$

```{r echo=F, figures-side2, fig.show="hold", out.width="50%", message=FALSE}
#Walker's
---
title: "Error Analysis"
author: "Walker Burgin"
date: "7/29/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(pracma)
library(xtable)
```

```{r}
h <- read.csv("/Users/walkerburgin/Downloads/hotel_bookings.csv")
h2 = h %>% 
  select(hotel, 
         arrival_date_month, 
         arrival_date_day_of_month, 
         arrival_date_year, 
         country,
         adr, 
         is_canceled) %>% 
  rename(tp = hotel, 
         ogn = country, 
         aM = arrival_date_month, 
         aD = arrival_date_day_of_month, 
         aY = arrival_date_year, 
         adr = adr)
month_levels <- c("January", "February", "March", "April", 
                  "May", "June", "July", "August", 
                  "September", "October", "November", "December")
repeat_levels <- c(0, 1)
h2$aM =  factor(h2$aM,levels = month_levels)
ct_h = h2 %>% 
  filter(tp == "City Hotel", is_canceled == 0) %>% 
  filter(ogn != "NULL")
rst_h = h2 %>% 
  filter(tp == "Resort Hotel", is_canceled == 0) %>% 
  filter(ogn != "NULL")
ct_h_freq = ct_h%>% 
  filter(adr != 0.00)%>% 
  group_by(ogn, aM) %>% 
  summarise(n = n()) %>% 
  mutate(frequency = n/sum(n))
rst_h_freq = rst_h %>% 
  filter(adr != 0.00)%>% 
  group_by(ogn, aM) %>% 
  summarise(n = n()) %>% 
  mutate(frequency = n/sum(n))
ct_h_adr = ct_h %>% 
  group_by(ogn, aM) %>% 
  arrange(ogn, aM) %>% 
  summarize_at(vars(adr),funs(mean(.,na.rm = TRUE) )) %>% 
  rename(amr = adr) 
rst_h_adr = rst_h %>% 
  group_by(ogn, aM) %>% 
  arrange(ogn, aM) %>% 
  summarize_at(vars(adr), funs(mean(., na.rm = TRUE) )) %>% 
  rename(amr = adr) 
ct_h_top = ct_h %>% 
  group_by(ogn) %>% 
  summarise(n = n()) %>% 
  arrange(desc(n))
ct_h_join = inner_join(ct_h_adr, ct_h_top, by = "ogn")
rst_h_top = rst_h %>% 
  group_by(ogn) %>% 
  summarise(n = n()) %>% 
  arrange(desc(n))
rst_h_join = inner_join(rst_h_adr, rst_h_top, by = "ogn")
ct_h_freq = ct_h_freq %>% 
  filter(n > 80, frequency < 0.6)

#City Filtering
h2 = h %>% 
  filter(hotel == "City Hotel", is_canceled == 0) %>% dplyr::select(arrival_date_year, arrival_date_month, arrival_date_day_of_month,stays_in_week_nights, stays_in_weekend_nights, adr) %>% mutate(length_of_stay = stays_in_week_nights + stays_in_weekend_nights) %>% arrange(arrival_date_year, arrival_date_month, arrival_date_day_of_month)

h2$arrival_date_month = as.integer(factor(h2$arrival_date_month, levels = month.name))

h2 = h2 %>% dplyr::select(arrival_date_year, arrival_date_month, arrival_date_day_of_month, length_of_stay, adr) %>% unite("arrival_date", c("arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"), sep = "/") %>% filter(adr != 0.00)

h2$arrival_date <- as.Date(h2$arrival_date)
h2 = h2 %>% arrange(arrival_date)  %>% group_by(arrival_date) %>% summarise(avg_length_of_stay = mean(length_of_stay), avg_adr = mean(adr))

#Resort Filtering
s2 = h %>% 
  filter(hotel == "Resort Hotel", is_canceled == 0) %>% dplyr::select(arrival_date_year, arrival_date_month, arrival_date_day_of_month,stays_in_week_nights, stays_in_weekend_nights, adr) %>% mutate(length_of_stay = stays_in_week_nights + stays_in_weekend_nights) %>% arrange(arrival_date_year, arrival_date_month, arrival_date_day_of_month)

s2$arrival_date_month = as.integer(factor(s2$arrival_date_month, levels = month.name))

s2 = s2 %>% dplyr::select(arrival_date_year, arrival_date_month, arrival_date_day_of_month, length_of_stay, adr) %>% unite("arrival_date", c("arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"), sep = "/") %>% filter(adr != 0.00)

s2$arrival_date <- as.Date(s2$arrival_date)
s2 = s2 %>% arrange(arrival_date)  %>% group_by(arrival_date) %>% summarise(avg_length_of_stay = mean(length_of_stay), avg_adr = mean(adr))

#CityData Graph
mod = h2
start_date = as.Date("2015-07-01")
mod$arrival_date <- as.numeric(difftime(mod$arrival_date, start_date, unit = "days"))
NumDays.city <- mod$arrival_date
xc <- cos(2*pi*NumDays.city/365.25)
xs <- sin(2*pi*NumDays.city/365.25)  
fit.lm <- lm(avg_adr ~ xc + xs + mod$arrival_date, data = mod)
model.func = function(day) {
  return(
    fit.lm$coefficients[[1]] + 
    fit.lm$coefficients[[2]]*cos(2*pi*day/365.25) + 
    fit.lm$coefficients[[3]]*sin(2*pi*day/365.25) + 
    fit.lm$coefficients[[4]]*day
  )
}
new = data.frame(arrival_date=793:1577)
mod = bind_rows(mod,new)
mod$pred <- model.func(mod$arrival_date)
mod$residual <- mod$avg_adr-mod$pred


resid <- remove_missing(mod,na.rm=TRUE,vars = names(mod))
resid$r2 <- resid$residual^2
RMSError <- sqrt(mean(resid$r2))
RMSUpperBound <-mod$pred
RMSLowerBound<-mod$pred
f_density<- function(x){
  (1+(x/1577))^2
}

ggplot()+
  geom_errorbar(
    mapping=aes(
      x=mod$arrival_date,
      y=mod$pred,
      ymax=mod$pred+(f_density(mod$arrival_date)*RMSError),
      ymin=mod$pred-(f_density(mod$arrival_date)*RMSError)),
    alpha=.025,
    color="red")+
  geom_point(
    mapping=aes(
      x=mod$arrival_date,
      y=mod$avg_adr),
    size=1,
    color="red",
    alpha=.25)+
  xlab("Time Elapsed Since 1 July 2015 (days)")+
  ylab("Average Adr")+
  theme_classic()+
  ggtitle("Forecasted Average ADR: City Hotels")+
  geom_vline(xintercept = 793,linetype=1,alpha=.05)+
  geom_line(
    mapping=aes(
      x=mod$arrival_date,
      y=mod$pred),
    color="purple",
    size=.5)+
  geom_smooth(
    mapping=aes(
      x=mod$arrival_date,
      y=mod$pred+(2*f_density(mod$arrival_date)*RMSError)),
      xseq=793:1577,
      se=FALSE,
      color="red",
      alpha=.25,
      linetype=1,
      size=.5
    )+
  geom_smooth(
    mapping=aes(
      x=mod$arrival_date,
      y=mod$pred-(2*f_density(mod$arrival_date)*RMSError)),
      xseq=793:1577,
      se=FALSE,
      color="red",
      alpha=.25,
      linetype=1,
    size=.5
    )+
  geom_vline(xintercept=365.25/2,color="red",linetype=3,size=.5)+
  geom_vline(xintercept=365.25*(3/2),color="red",linetype=3,size=.5)+
  geom_vline(xintercept=365.25*(5/2),color="red",linetype=3,size=.5)+
  geom_vline(xintercept=365.25*(7/2),color="red",linetype=3,size=.5)
ggplot()+theme_classic()+ggtitle("Accuracy of Predicted Curve")+ylab("Average ADR")+xlab("Arrival Date")+geom_area(mapping=aes(x=resid$arrival_date,y=resid$residual),fill="red")+geom_hline(yintercept = RMSError,linetype=2)

#Resort Data Graph
modb = s2
start_date = as.Date("2015-07-01")
modb$arrival_date <- as.numeric(difftime(modb$arrival_date, start_date, unit = "days"))
NumDays.city <- modb$arrival_date
xc <- cos(2*pi*NumDays.city/365.25)
xs <- sin(2*pi*NumDays.city/365.25)  
fit.lm <- lm(avg_adr ~ xc + xs + modb$arrival_date, data = modb)
model.func = function(day) {
  return(
    fit.lm$coefficients[[1]] + 
    fit.lm$coefficients[[2]]*cos(2*pi*day/365.25) + 
    fit.lm$coefficients[[3]]*sin(2*pi*day/365.25) + 
    fit.lm$coefficients[[4]]*day
  )
}
new = data.frame(arrival_date=793:1577)
modb = bind_rows(modb,new)
modb$pred <- model.func(modb$arrival_date)
modb$residual <- modb$avg_adr-modb$pred


resid <- remove_missing(modb,na.rm=TRUE,vars = names(modb))
resid$r2 <- resid$residual^2
RMSError <- sqrt(mean(resid$r2))
RMSUpperBound <-modb$pred
RMSLowerBound<-modb$pred
f_density<- function(x){
  (1+(x/1577))^2
}

ggplot()+
  geom_errorbar(
    mapping=aes(
      x=modb$arrival_date,
      y=modb$pred,
      ymax=modb$pred+(f_density(modb$arrival_date)*RMSError),
      ymin=modb$pred-(f_density(modb$arrival_date)*RMSError)),
    alpha=.025,
    color="red")+
  geom_point(
    mapping=aes(
      x=modb$arrival_date,
      y=modb$avg_adr),
    size=1,
    color="red",
    alpha=.25)+
  xlab("Time Elapsed Since 1 July 2015 (days)")+
  ylab("Average Adr")+
  theme_classic()+
  ggtitle("Forecasted Average ADR: Resort Hotels")+
  geom_vline(xintercept = 793,linetype=1,alpha=.05)+
  geom_line(
    mapping=aes(
      x=modb$arrival_date,
      y=modb$pred),
    color="purple",
    size=.5)+
  geom_smooth(
    mapping=aes(
      x=modb$arrival_date,
      y=modb$pred+(2*f_density(modb$arrival_date)*RMSError)),
      xseq=793:1577,
      se=FALSE,
      color="red",
      alpha=.25,
      linetype=1,
      size=.5
    )+
  geom_smooth(
    mapping=aes(
      x=modb$arrival_date,
      y=modb$pred-(2*f_density(modb$arrival_date)*RMSError)),
      xseq=793:1577,
      se=FALSE,
      color="red",
      alpha=.25,
      linetype=1,
    size=.5
    )+
  geom_vline(xintercept=365.25/2,color="red",linetype=3,size=.5)+
  geom_vline(xintercept=365.25*(3/2),color="red",linetype=3,size=.5)+
  geom_vline(xintercept=365.25*(5/2),color="red",linetype=3,size=.5)+
  geom_vline(xintercept=365.25*(7/2),color="red",linetype=3,size=.5)
ggplot()+theme_classic()+ggtitle("Accuracy of Predicted Curve")+ylab("Average ADR")+xlab("Arrival Date")+geom_area(mapping=aes(x=resid$arrival_date,y=resid$residual),fill="red")+geom_hline(yintercept = RMSError,linetype=2)
```

Our prediction model is displayed in "Forecasted Average ADR", with confidence intervals defined by the RMSE of 13.31 for City Hotels and 27.79 for Resort Hotels. Our second chart, "Residuals of Predicted Curve", identifies which segments of the prediction model are less accurate than others at mapping the dataset.

## Question 2: Which model best predicts cancellation?

### How do significant variables correlate with cancellation?

To determine which variables we should use in our models, we started by identifying the variables that had a significant relationship with cancellation. Pictured are graphs showing the significant variables' relationship with cancellation, along with a table of P values. The P values for the first 4 variables were calculated using a T-test. The P values for the last 2 were calculated using a two proportion Z-test.

<center>

##### Relationship between is_canceled and other significant variables

</center>

```{r, echo=F, figures-side3, fig.show="hold", out.width="50%"}
leadgraph = df %>% 
  group_by(as.logical(is_canceled)) %>%
  summarize(avg_lead_time = mean(lead_time)) %>% 
  ungroup() %>%
  rename(value=avg_lead_time)
leadgraph$key=rep("Lead Time", 2)

prevgraph = df %>%
  group_by(as.logical(is_canceled)) %>%
  summarize(avg_prev_canc = mean(previous_cancellations)) %>% 
  ungroup() %>%
  rename(value=avg_prev_canc)
prevgraph$key=rep("Previous Cancellations", 2)

prevuncancgraph = df %>%
  group_by(as.logical(is_canceled)) %>%
  summarize(avg_prev_uncanc = mean(previous_bookings_not_canceled)) %>% 
  ungroup() %>%
  rename(value=avg_prev_uncanc)
prevuncancgraph$key=rep("Previous Bookings Not Canceled", 2)

adrgraph = df %>%
  group_by(as.logical(is_canceled)) %>%
  summarize(avg_adr = mean(adr)) %>% 
  ungroup() %>%
  rename(value=avg_adr)
adrgraph$key=rep("ADR", 2)

fullgraph = bind_rows(leadgraph, prevgraph, prevuncancgraph, adrgraph)

#Graph
ggplot(fullgraph, aes(x=`as.logical(is_canceled)`, y=value)) + 
  geom_col(fill="slateblue") + 
  facet_wrap(~key, scales = "free") + 
  xlab("Is Canceled?") + 
  ylab("Mean Value")
  
hotelgraph = df %>%
  group_by(hotel) %>%
  summarize(
    n=n(),
    num_canceled = sum(is_canceled),
    prop_canceled = mean(is_canceled)
  ) %>%
  ungroup() %>%
  rename(value=hotel)
hotelgraph$key=rep("Hotel", 2)

repeatgraph = df %>%
  group_by(as.logical(is_repeated_guest)) %>%
  summarize(
    n=n(),
    num_canceled = sum(is_canceled),
    prop_canceled = mean(is_canceled)
  ) %>%
  ungroup() %>%
  rename(value=`as.logical(is_repeated_guest)`)
repeatgraph$key=rep("Is Repeated Guest?", 2)
repeatgraph$value=as.character(repeatgraph$value)

fullgraph2 = bind_rows(hotelgraph, repeatgraph)

#Graph
ggplot(fullgraph2, aes(x=value, y=prop_canceled)) + 
  geom_col(fill="slateblue") + 
  facet_wrap(~key, scales="free") + 
  xlab("Value") + 
  ylab("Proportion Canceled")
```

```{r, results="asis", echo=F}
pvals = c()

pvals = c(
  t.test(df$lead_time~as.logical(df$is_canceled))$p.value[[1]],
  t.test(df$previous_cancellations~as.logical(df$is_canceled))$p.value[[1]],
  t.test(df$previous_bookings_not_canceled~as.logical(df$is_canceled))$p.value[[1]],
  t.test(df$adr~as.logical(df$is_canceled))$p.value[[1]],
  (prop.test(x=hotelgraph$num_canceled, 
             n=hotelgraph$n,
             p=NULL,
             alternative = "two.sided",
             correct=TRUE))$p.value[[1]],
  (prop.test(x=repeatgraph$num_canceled, 
             n=repeatgraph$n,
             p=NULL,
             alternative = "two.sided",
             correct=TRUE))$p.value[[1]]
)
pvals=formatC(x=pvals, format="e", digits=2)

vars = c("Average Lead Time",
         "Average Previous Cancellations",
         "Average Previous Bookings Not Canceled",
         "Average ADR",
         "Hotel",
         "Is Repeated Customer?")

pvaltable = data.frame(Variables = vars, "P values" = pvals)
pvaltable$P.values = as.character(pvaltable$P.values)
to_print = pvaltable %>% 
  xtable(align="ccc", caption = "Table 3: P Values from 2-proportion Z Test")
print(to_print, 
      "html", 
      caption.placement = "top",
      scalebox = 0.7,
      include.rownames=F,
      html.table.attributes="align='center',
                             rules='rows',
                             width=50%,
                             frame='hsides',
                             border-spacing=5px"
      )
```

### Model Selection: 

Using the significant variables, we created multiple predictive models: logistic, step-wise logistic, logistic with two-fold interaction, k-NN, and RandomForest. To provide an example of one of our models, Table 4 gives coefficients for the simple logistic model. 

```{r, include=F}
#Data cleaning
library(MASS)
df_log = df %>%
  dplyr::select(hotel,
         is_canceled,
         lead_time,
         previous_cancellations,
         previous_bookings_not_canceled,
         adr,
         is_repeated_guest
         ) %>% mutate(id=row_number())

set.seed(216)
df_train=df_log %>% 
  sample_frac(0.80)
df_test=anti_join(df_log, df_train, by='id')
df_train= df_train %>%
  subset(select = -id)
df_test= df_test %>%
  subset(select= -id)
```

```{r, include=F}
#Model 1: Logistic
model1 = glm(
  is_canceled~., 
  family="binomial",
  data=df_train)

options("scipen"=100, "digits"=4)
summary(model1)

df_test = df_test %>% add_predictions(
  model1,
  var="predicted_canc1") %>%
  mutate(predicted_canc1 = ifelse(predicted_canc1 > 0.5,1,0))
```

```{r, results="asis", echo=F}
log_table = tidy(model1)[,c("term", "estimate", "p.value")] %>%
  rename(Term="term",
         Estimate="estimate",
         P.value="p.value") %>%
  mutate(Estimate=format(round(Estimate, 4), nsmall=4),
         P.value=formatC(P.value, format="e", digits = 3))

to_print_log_mod = log_table %>% 
  xtable(align="cccc", caption = "Table 4: Coefficients of Simple Logistic Model")
print(to_print_log_mod, 
      "html", 
      caption.placement = "top",
      scalebox = 0.7,
      include.rownames=F,
      html.table.attributes="align='center',
                             rules='rows',
                             width=50%,
                             frame='hsides',
                             border-spacing=5px"
)
```

```{r, include=F, eval=F}
#Model 2: Stepwise logistic
model2 = stepAIC(model1)

df_test = df_test %>% add_predictions(
  model2,
  var="predicted_canc2") %>%
  mutate(predicted_canc2 = ifelse(predicted_canc2 > 0.5,1,0))
```

```{r, include=F, eval=F}
#Model 3: Logistic with twofold interaction
model3 = glm(
  is_canceled~.^2, 
  family="binomial",
  data=df_train)
tidy(model3)[,c("term", "estimate", "p.value")] #make tables using these tibbles?

df_test = df_test %>% add_predictions(
  model3,
  var="predicted_canc3") %>%
  mutate(predicted_canc3 = ifelse(predicted_canc3 > 0.5,1,0))
```

```{r, echo=F}
#standardize dataset for k-NN and RandomForest
standardize = function(vector) {
  return(sd(vector)*vector + 
           mean(vector)
              )
}

#Sid's accuracy fxn
#accuracy = function(x) {
#  sum(diag(x)/(sum(rowSums(x)))) * 100
#}

#input df_test$predicted_canc1, or 2, etc.
sensitivity = function(predicted_canc) {
  return(
    sum(df_test$is_canceled & predicted_canc) /
      sum(df_test$is_canceled)
  )
}

specificity = function(predicted_canc) {
  return(
    sum(!df_test$is_canceled & predicted_canc==0) /
      sum(!df_test$is_canceled)
  )
}

accuracy = function(predicted_canc) {
  (
  sum(df_test$is_canceled & predicted_canc) + 
    sum(!df_test$is_canceled & predicted_canc==0)
  ) / nrow(df_test)
}
```

```{r, eval=T, include=F}
#Model4: k-NN, k=11

df_knn = df_test %>%
  mutate(
    previous_cancellations=standardize(previous_cancellations),
    previous_bookings_not_canceled=standardize(previous_bookings_not_canceled),
    hotel=ifelse(hotel=="Resort Hotel", 1, 0),
    hotel=standardize(hotel),
    lead_time=standardize(lead_time),
    adr=standardize(adr)
    ) %>% 
  mutate(predicted_canc4=knn(
    train=dplyr::select(
      df_train,
      lead_time,
      adr,
      previous_cancellations
      ),
    test=dplyr::select(
      df_test,
      lead_time,
      adr,
      previous_cancellations
    ),
    cl=
      factor(
      df_train$is_canceled,
      levels=c(0,1),
      labels=c("0","1")
    ),
    k=11)
  ) %>%
  dplyr::select(predicted_canc4)

df_test = bind_cols(df_test, df_knn) %>%
  mutate(predicted_canc4 = as.integer(predicted_canc4)-1)
```

```{r, eval=T, echo=F}
#RandomForest
rf = randomForest(is_canceled~.,
                  data = df_train,
                  ntree=30)
predicted = as.integer(predict(rf, newdata=df_test))-1
conf_mat = table(predicted, df_test$is_canceled)
df_test = df_test %>%
  mutate(predicted_canc5 = predicted)
```

<br>

As for the k-NN and RandomForest models, we needed to determine which k and ntree values we should use. To determine which k value we should use for our k-NN model, we analyzed changes in accuracy, sensitivity, and specificity over time. In the context of our situation, it is more important that hotels correctly classify those who did not cancel, because we want to minimize the hassle from overbooking rooms that weren't actually canceled. However, we still want to maintain accuracy. Thus, we care more about accuracy and specificity than sensitivity. Accordingly, we chose k=11, because it maximizes accuracy$\times$specificity. Below is a visualization of this analysis. As for the RandomForest model, there was little variation of metrics by ntree value. Nevertheless, we chose parameter ntree=30 because it maximized accuracy$\times$specificity.

<br>

```{r, echo=F, figures-side4, fig.show="hold", out.width="50%"}
k_df_plot = k_df %>% 
  rename(
    "Accuracy"="acc",
    "Sensitivity"="sens",
    "Specificity"="spec"
  ) %>%
  gather(
    "Accuracy":"Specificity", 
    key="Metric", 
    value="Value"
    )

ggplot(k_df_plot) + 
  geom_line(aes(x=k, y=Value, color=Metric)) + 
  ggtitle("Performance of k-NN model by k value") + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  geom_vline(xintercept=11, color="black", size=0.5)

ggplot(rf_stats %>% rename(Metric=metric)) + 
  geom_line(aes(x=n, y=value, color=Metric)) + 
  geom_vline(xintercept=30, color="black", size=0.5) + 
  ggtitle("Performance of RandomForest Model by ntree Value") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  xlab("ntree") + 
  ylab("Value")
```

The logistic models performed the same in terms of accuracy, sensitivity, and specificity, so we consider only the simple logistic model here. We compared simple logistic, k-NN with k=11, and RandomForest with ntree=30. Table 5 gives the metrics of our models' performance. 

```{r, results="asis", echo=F}

metrics_table = tribble(
  ~Model, ~Sensitivity, ~Specificity, ~Accuracy, ~Spec_Times_Accuracy,
  1, sensitivity(df_test$predicted_canc1), specificity(df_test$predicted_canc1), accuracy(df_test$predicted_canc1), specificity(df_test$predicted_canc1)*accuracy(df_test$predicted_canc1),
  2, k_df$sens[[11]], k_df$spec[[11]], k_df$acc[[11]], k_df$spec[[11]]*k_df$acc[[11]],
  3, 0.25, 0.99, 0.72, 0.71
) %>% mutate(Model=as.integer(Model))

to_print1 = metrics_table %>% 
  xtable(align="cccccc", caption = "Table 5: Model Performance Metrics")
print(to_print1, 
      "html", 
      include.rownames=F,
      html.table.attributes="align='center',
                             rules='rows',
                             width=50%,
                             frame='hsides',
                             border-spacing=5px"
)
```

```{r, include=F}
detach("package:MASS", unload = TRUE)
```

# Conclusion

To answer our first question, which predicts the average daily rate for both the city and the resort hotels for dates not in the dataset, we first built a model on the data that we had in the data set. We noticed that the data trended in an oscillation - the average daily rate per day increased during the summer months and decreased during winter months. The peaks and troughs in this seasonality resembled a sinusoidal graph, so that's how we modeled the data. From 2015 to 2017, the peaks and troughs also gradually increased, indicating that the hotels were getting slightly more expensive per year. To account for the increasing rates - which could be due to inflation, we added a linear component of days elapsed from the inception of our model. Thus, we combined the linear and sinusoidal components to create the most accurate model for predicting booking prices. 

To model a line of best fit using the sinusoidal nature of the graph, we first converted the days elapsed into sin and cos values and then used a fitting function on the data to find the coefficients for each trigonometry function. Once we found the coefficients, we were able to predict the prices for future dates, not in the data set or out-of-sample. To do this, we created a specific model function, which used the coefficients determined from the previous fit to determine the line of best fit for future dates. Using the model function described earlier, we were able to predict average daily rates for the next two years by arrival date (elapsed time). As the trend line shows, the next two years follow a similar sine curve and also shows a positive trend upward in the price for both the resort and city hotels - however, this trend is more pronounced for city hotels.

The city hotel prediction model indicates that the average daily rate per day is higher during the summer months than winter months, as the difference in price between the peak price (usually in July) and the lowest price (usually in January) within a year is around 60 - 70 units. However, this difference is much severer at resort hotels. The resort hotels follow a similar pattern, where the average daily rate is higher in summer months than in winter months, but the range between the highest and lowest values within the year is 100 - 125 units. The data is unclear on whether the price is measured in Euros (currency of Portugal), or US Dollars, so for analysis of the data, “units” would be a more appropriate term for the measurement of money. This larger difference in resort hotel data is also accompanied by a steeper decline, so that indicates that the price remains high for a shorter amount of time than the city hotels. This also contributes to the fact that city hotel data is more rounded and looks like a series of hills, while the resort data looks sharper and looks more like a series of mountains. 

Resort hotels specifically targeting customers looking for a vacation - which usually occurs during the summer months, can explain the sharper prices. Contrastly, city hotels can have vacationers or customers traveling for work-related or other reasons. Either way, the prediction model helps customers minimize costs and see when prices will be the highest and what seasons to keep an eye on for lower average daily rates. Contrastly, the model will help hotel management maximize profit and determine when their revenues from their hotel rates are expected to be at their highest or lowest and consequently help them prepare for shifts in consumer demand.


To answer which model best predicts customer cancellations, we first had to find which variables would be most significant in predicting cancellations. After conducting T-Tests and a 2 proportion Z-test we found average lead time, average previous cancellations, average previous bookings not canceled, average adr, hotel, and is repeated customer to be statistically significant. When testing different combinations of these variables in a logistic regression model, we found that regardless of the complexity, all the Logistic Regression models performed at identical rates across accuracy, sensitivity, and specificity metrics. Our group decided to use the simplest Logistic Regression model's scores when comparing it to the next two machine learning models: K Nearest Neighbors(K-NN) and Random Forest classification.

Since the k nearest neighbors model uses the distance formula to classify whether a customer will cancel, we normalized our numeric variables so that one feature did not weigh too heavily when calculating the distance measures. Our group found that as k increased, the specificity increased while the sensitivity rapidly decreased. To give equal weight to both accuracy and specificity, we generated a new metric (specificityXaccuracy) to minimize double bookings for hotel managers. We found that K-NN where k=11 maximized this metric. Lastly, we generated the Random Forest Classification model that ensembles many decision trees to predict whether a customer would cancel or not. We found that when there were 30 decision trees (ntree=30) the metric was maximized. 

In the end, the Random Forest model performed the best with a specificityXaccuracy score of 0.71 The logistic regression performed slightly better than the K-NN with a specificityXaccuracy score of 0.67 versus a 0.66 Predicting cancelations has a twofold benefit for both hotel managers and customers. Hotel managers benefit by allocating resources better and not double booking rooms that are going to have customers arrive. While customers benefit by getting access to waitlists and discounts for bookings that are more likely to be canceled. In the future, to fine-tune our models, we would perform hyperparameter tuning for all three machine learning algorithms. By employing a random grid search and specifying ranges for the different hyperparameters for our models, we could calculate scores and select the parameters for a trained model to perform the best on out of sample data.

